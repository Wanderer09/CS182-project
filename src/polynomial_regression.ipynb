{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
    "from plot_utils import basic_plot, collect_results, relevant_model_names\n",
    "from samplers import get_data_sampler\n",
    "from tasks import get_task_sampler\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (101) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m run_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, task, run_id)\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m read_run_dir(run_dir)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mget_run_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m metrics \u001b[38;5;241m=\u001b[39m collect_results(run_dir, df, valid_row\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m r: r\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m task \u001b[38;5;129;01mand\u001b[39;00m r\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;241m==\u001b[39m run_id)\n\u001b[1;32m     11\u001b[0m _, conf \u001b[38;5;241m=\u001b[39m get_model_from_run(run_path, only_conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/in-context-learning/src/eval.py:318\u001b[0m, in \u001b[0;36mget_run_metrics\u001b[0;34m(run_path, step, cache, skip_model_load, skip_baselines)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m checkpoint_created \u001b[38;5;241m>\u001b[39m cache_created:\n\u001b[1;32m    316\u001b[0m         recompute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m all_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_evals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_metrics\n",
      "File \u001b[0;32m~/in-context-learning/src/eval.py:280\u001b[0m, in \u001b[0;36mcompute_evals\u001b[0;34m(all_models, evaluation_kwargs, save_path, recompute)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m metrics \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m recompute:\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         metrics[model\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     all_metrics[eval_name] \u001b[38;5;241m=\u001b[39m metrics\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/in-context-learning/src/eval.py:184\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, task_name, data_name, n_dims, n_points, prompting_strategy, num_eval_examples, batch_size, data_sampler_kwargs, task_sampler_kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_eval_examples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size):\n\u001b[1;32m    182\u001b[0m     xs, xs_p \u001b[38;5;241m=\u001b[39m generating_func(data_sampler, n_points, batch_size)\n\u001b[0;32m--> 184\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43meval_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     all_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m    187\u001b[0m metrics \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_metrics, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/in-context-learning/src/eval.py:51\u001b[0m, in \u001b[0;36meval_batch\u001b[0;34m(model, task_sampler, xs, xs_p)\u001b[0m\n\u001b[1;32m     49\u001b[0m     ys \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(xs)\n\u001b[1;32m     50\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(xs\u001b[38;5;241m.\u001b[39mto(device), ys\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 51\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     b_size, n_points, _ \u001b[38;5;241m=\u001b[39m xs\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/in-context-learning/src/tasks.py:7\u001b[0m, in \u001b[0;36msquared_error\u001b[0;34m(ys_pred, ys)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msquared_error\u001b[39m(ys_pred, ys):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mys_pred\u001b[49m)\u001b[38;5;241m.\u001b[39msquare()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (101) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "run_dir = \"../models\"\n",
    "task = \"polynomial_regression_1dim\"\n",
    "run_id = \"pretrained\"  # 或你训练过的任意 run_id\n",
    "\n",
    "run_path = os.path.join(run_dir, task, run_id)\n",
    "df = read_run_dir(run_dir)\n",
    "\n",
    "get_run_metrics(run_path)\n",
    "\n",
    "metrics = collect_results(run_dir, df, valid_row=lambda r: r.task == task and r.run_id == run_id)\n",
    "_, conf = get_model_from_run(run_path, only_conf=True)\n",
    "\n",
    "models = relevant_model_names[\"polynomial_regression\"]  # 适配绘图\n",
    "\n",
    "print(\"Available models in metrics['standard']:\", metrics[\"standard\"].keys())\n",
    "\n",
    "basic_plot(metrics[\"standard\"], models=models)\n",
    "plt.title(f\"PolynomialRegression - {run_id}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = conf.model.n_dims\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    if name == \"standard\": continue\n",
    "\n",
    "    if \"scale\" in name:\n",
    "        scale = float(name.split(\"=\")[-1])**2\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    fig, ax = basic_plot(metric, models=models, trivial=1.0 * scale)\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylim(-.1 * scale, 1.5 * scale)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines_1dim import (\n",
    "    zero_estimator,\n",
    "    mean_estimator,\n",
    "    linear_regression_estimator,\n",
    "    polynomial_fit_estimator,\n",
    ")\n",
    "\n",
    "# 模型预测\n",
    "with torch.no_grad():\n",
    "    transformer_pred = model(xs, ys)\n",
    "\n",
    "# 各种 baseline 预测\n",
    "zero_pred = zero_estimator(xs, ys, xs)\n",
    "mean_pred = mean_estimator(xs, ys, xs)\n",
    "linear_pred = linear_regression_estimator(xs, ys, xs)\n",
    "oracle_pred = polynomial_fit_estimator(xs, ys, xs, degree=conf.training.task_kwargs[\"degree\"])\n",
    "\n",
    "# 统一评估\n",
    "metric = task.get_metric()\n",
    "loss_transformer = metric(transformer_pred, ys).numpy()\n",
    "loss_zero = metric(zero_pred, ys).numpy()\n",
    "loss_mean = metric(mean_pred, ys).numpy()\n",
    "loss_linear = metric(linear_pred, ys).numpy()\n",
    "loss_oracle = metric(oracle_pred, ys).numpy()\n",
    "\n",
    "# 可视化比较\n",
    "plt.plot(loss_transformer.mean(axis=0), label=\"Transformer\")\n",
    "plt.plot(loss_zero.mean(axis=0), label=\"Zero Estimator\")\n",
    "plt.plot(loss_mean.mean(axis=0), label=\"Mean Estimator\")\n",
    "plt.plot(loss_linear.mean(axis=0), label=\"Linear Regression\")\n",
    "plt.plot(loss_oracle.mean(axis=0), label=\"Oracle Poly Fit\")\n",
    "plt.xlabel(\"# in-context examples\")\n",
    "plt.ylabel(\"squared error\")\n",
    "plt.legend()\n",
    "plt.title(\"Baselines vs Transformer\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in-context-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
